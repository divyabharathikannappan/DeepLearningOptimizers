# DeepLearningOptimizers
This project aims to evaluate the performance of diff erent optimizers (Adam, RMSprop, AdamW) with varying learning rates (0.001, 0.0005) on the KMNIST dataset using a feedforward neural network. To ensure robust results, the evaluation uses 5-fold cross-validation.
